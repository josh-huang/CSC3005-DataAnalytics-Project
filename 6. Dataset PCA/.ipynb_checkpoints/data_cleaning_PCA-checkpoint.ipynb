{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee31ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data processing, modeling, and model evaluation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, plot_confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# Randomization\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e271bb98",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '5. Dataset Feature Dropping/MeterA_featureSelection.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fdeaf682cb46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load and check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdataframeA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'5. Dataset Feature Dropping/MeterA_featureSelection.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#display(dataframeA.head())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataframeA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '5. Dataset Feature Dropping/MeterA_featureSelection.csv'"
     ]
    }
   ],
   "source": [
    "# Load and check data\n",
    "\n",
    "dataframeA = pd.read_csv('5. Dataset Feature Dropping/MeterA_featureSelection.csv')\n",
    "#display(dataframeA.head())\n",
    "dataframeA.head()\n",
    "\n",
    "dataframeA = dataframeA.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "featuresVector = dataframeA[['Flatness ratio','Symmetry','Crossflow','Average speed of sound','Average Gain']]\n",
    "\n",
    "# yLbl is Class Attruibute labels\n",
    "\n",
    "yLbl = dataframeA[['Class Attruibute']]\n",
    "\n",
    "#featuresVector\n",
    "# featuresVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeA['Class Attruibute'].value_counts()\n",
    "featuresVector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8534964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scaler object (model)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "scaled_data = scaler.fit_transform(featuresVector) \n",
    "#scaled_data1 = scaler.fit_transform(dataframeA)\n",
    "\n",
    "#print(dataframeA)\n",
    "print(\"------------------------------  scaled data for A -----------------------------\")\n",
    "print(scaled_data)\n",
    "\n",
    "#print(\"-----------------------------------------------------------\")\n",
    "#print(scaled_data[:6])\n",
    "\n",
    "#95% of variance\n",
    "#to select the number of components\n",
    "\n",
    "#pca = PCA(n_components = 0.95)\n",
    "#pca.fit(scaled_data)\n",
    "#reduced = pca.transform(scaled_data)\n",
    "\n",
    "#reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e66df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.mikulskibartosz.name/pca-how-to-choose-the-number-of-components/\n",
    "pca = PCA().fit(scaled_data)\n",
    "\n",
    "# matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xi = np.arange(1, 6, step=1)\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, 6, step=1)) #change from 0-based array index to 1-based human-readable label\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "ax.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cumulative sum of explained variances\n",
    "#from lab \n",
    "pca = PCA().fit(scaled_data)\n",
    "\n",
    "tot = sum(pca.explained_variance_)\n",
    "print(\"---------------PCA - DATA A : EXPLAIN VARIANCE--------------\")\n",
    "print(pca.explained_variance_)\n",
    "print(\"---------------PCA - DATA A : SUM VARIANCE--------------\")\n",
    "print(tot)\n",
    "var_exp = [(i / tot) for i in sorted(pca.explained_variance_, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"---------------PCA - DATA A : SIZE OF VARIANCE --------------\")\n",
    "print(len(var_exp))\n",
    "# plot explained variances\n",
    "plt.bar(range(1,6), var_exp, alpha=0.5,\n",
    "        align='center', label='individual explained variance')\n",
    "plt.step(range(1,6), cum_var_exp, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#only the first two components capture almost all the variance in the dataset. \n",
    "#So, we decide to select only the first two components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e60060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.jcchouinard.com/pca-with-python/\n",
    "#from sklearn.decomposition import PCA\n",
    "sns.set()\n",
    " \n",
    "pca = PCA(n_components=5)\n",
    "#pca = PCA().fit(scaled_data)\n",
    " \n",
    "# Fit and transform data\n",
    "reduced_features = pca.fit_transform(scaled_data)\n",
    " \n",
    "# Bar plot of explained_variance\n",
    "plt.bar(\n",
    "    range(1,len(pca.explained_variance_)+1),\n",
    "    pca.explained_variance_\n",
    "    )\n",
    " \n",
    "plt.plot(\n",
    "    range(1,len(pca.explained_variance_ )+1),\n",
    "    np.cumsum(pca.explained_variance_),\n",
    "    c='red',\n",
    "    label='Cumulative Explained Variance')\n",
    " \n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance (eignenvalues)')\n",
    "plt.title('Scree plot')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5e7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the diagrams aboved, only the first two components capture almost all the variance in the dataset. \n",
    "#So, we decide to select only the first two components for PCA (DataSet A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7433639",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "\n",
    "pca_features = pca.fit_transform(scaled_data)\n",
    " \n",
    "print('Shape before PCA: ', scaled_data.shape)\n",
    "print('Shape after PCA: ', pca_features.shape)\n",
    " \n",
    "pca_df = pd.DataFrame(\n",
    "    data=pca_features, \n",
    "    columns=['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "\n",
    "finalDf_A = pd.concat([pca_df, dataframeA['Class Attruibute']], axis = 1)\n",
    "\n",
    "print(\"------------------------------  PCA For A  -----------------------------\")\n",
    "finalDf_A\n",
    "finalDf_A.to_csv('MeterA_pca.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddde31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(pca_features[:,0],pca_features[:,1],pca_features[:,2],pca_features[:,3],c=dataframeA['Class Attruibute'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b917b09",
   "metadata": {},
   "source": [
    "# DATA SET B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset B\n",
    "\n",
    "# Load and check data\n",
    "\n",
    "dataframeB = pd.read_csv('5. Dataset Feature Dropping/MeterB_featureSelection.csv')\n",
    "#display(dataframeA.head())\n",
    "dataframeB.head()\n",
    "\n",
    "dataframeB = dataframeB.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "featuresVector_B = dataframeB[['Profile factor','Symmetry','Crossflow','Swirl Angle','Average flow velocity','Average speed of sound','Average Signal Strength','Average Signal Strength','Meter performance','Average Gain','Average Transit Time']]\n",
    "\n",
    "# yLbl is Class Attruibute labels\n",
    "\n",
    "yLblForB = dataframeB[['Class Attruibute']]\n",
    "\n",
    "#featuresVector\n",
    "featuresVector_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scaler object (model)\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "scaled_data_B = scaler.fit_transform(featuresVector_B) \n",
    "#scaled_data1 = scaler.fit_transform(dataframeA)\n",
    "\n",
    "#print(dataframeA)\n",
    "print(\"------------------------------  scaled data for B -----------------------------\")\n",
    "print(scaled_data_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape before PCA: ', scaled_data_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2025bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cumulative sum of explained variances\n",
    "pcaB = PCA().fit(scaled_data_B)\n",
    "\n",
    "# matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xi = np.arange(1, 12, step=1)\n",
    "y = np.cumsum(pcaB.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, 12, step=1)) #change from 0-based array index to 1-based human-readable label\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "ax.grid(axis='x')\n",
    "plt.show()\n",
    "\n",
    "#components needed: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cumulative sum of explained variances\n",
    "#from Lab\n",
    "pcaB = PCA().fit(scaled_data_B)\n",
    "\n",
    "total_B = sum(pcaB.explained_variance_)\n",
    "print(\"---------------PCA - DATA B : EXPLAIN VARIANCE--------------\")\n",
    "print(pcaB.explained_variance_)\n",
    "print(\"---------------PCA - DATA B : SUM VARIANCE--------------\")\n",
    "print(total_B)\n",
    "var_exp_B = [(i / tot) for i in sorted(pcaB.explained_variance_, reverse=True)]\n",
    "cum_var_exp_B = np.cumsum(var_exp_B)\n",
    "print(\"---------------PCA - DATA A : SIZE OF VARIANCE --------------\")\n",
    "print(len(var_exp_B))\n",
    "# plot explained variances\n",
    "plt.bar(range(1,12), var_exp_B, alpha=0.5,\n",
    "        align='center', label='individual explained variance')\n",
    "plt.step(range(1,12), cum_var_exp_B, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a3f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.jcchouinard.com/pca-with-python/\n",
    "sns.set()\n",
    " \n",
    "# Reduce from 5 to 3 features with PCA\n",
    "pcaB = PCA(n_components=10)\n",
    " \n",
    "# Fit and transform data\n",
    "reduced_features = pcaB.fit_transform(scaled_data_B)#featuresVector_B)\n",
    " \n",
    "# Bar plot of explained_variance\n",
    "plt.bar(\n",
    "    range(1,len(pcaB.explained_variance_)+1),\n",
    "    pcaB.explained_variance_\n",
    "    )\n",
    " \n",
    "plt.plot(\n",
    "    range(1,len(pcaB.explained_variance_ )+1),\n",
    "    np.cumsum(pcaB.explained_variance_),\n",
    "    c='red',\n",
    "    label='Cumulative Explained Variance')\n",
    " \n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance (eignenvalues)')\n",
    "plt.title('Scree plot For B')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "#pca = 3\n",
    "\n",
    "#only the first 3 components capture almost all the variance in the dataset. \n",
    "#So, we decide to select only the first 3 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_B = PCA(n_components=4)\n",
    "\n",
    "pca_features_B = pca_B.fit_transform(scaled_data_B)\n",
    " \n",
    "print('Shape before PCA: ', scaled_data_B.shape)\n",
    "print('Shape after PCA: ', pca_features_B.shape)\n",
    " \n",
    "pca_df_B = pd.DataFrame(\n",
    "    data=pca_features_B, \n",
    "    columns=['PC1', 'PC2', 'PC3','PC4'])\n",
    "\n",
    "finalDf_B = pd.concat([pca_df_B, dataframeB['Class Attruibute']], axis = 1)\n",
    "\n",
    "print(\"------------------------------  PCA For B  -----------------------------\")\n",
    "finalDf_B\n",
    "finalDf_B.to_csv('MeterB_pca.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad5387",
   "metadata": {},
   "source": [
    "# PCA FOR DATA SET C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset B\n",
    "\n",
    "# Load and check data\n",
    "\n",
    "dataframeC = pd.read_csv('5. Dataset Feature Dropping/MeterC_featureSelection.csv')\n",
    "#display(dataframeA.head())\n",
    "dataframeC.head()\n",
    "\n",
    "dataframeC = dataframeC.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "featuresVector_C = dataframeC[['Profile factor','Symmetry','Crossflow','Average Flow Velocity','Average Gain','Average Signal Quality','Average Signal Strength']]\n",
    "\n",
    "# yLbl is Class Attruibute labels\n",
    "\n",
    "yLblForC = dataframeC[['Class Attruibute']]\n",
    "\n",
    "#featuresVector\n",
    "featuresVector_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb74109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform the data\n",
    "scaled_data_C = scaler.fit_transform(featuresVector_C) \n",
    "#scaled_data1 = scaler.fit_transform(dataframeA)\n",
    "\n",
    "#print(dataframeA)\n",
    "print(\"------------------------------  scaled data for C -----------------------------\")\n",
    "print(scaled_data_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape before PCA: ', scaled_data_C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cumulative sum of explained variances\n",
    "pcaC = PCA().fit(scaled_data_C)\n",
    "\n",
    "# matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xi = np.arange(1, 8, step=1)\n",
    "y = np.cumsum(pcaC.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, 8, step=1)) #change from 0-based array index to 1-based human-readable label\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "ax.grid(axis='x')\n",
    "plt.show()\n",
    "\n",
    "#components needed: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484154b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cumulative sum of explained variances\n",
    "#from lab\n",
    "\n",
    "pcaC = PCA().fit(scaled_data_C)\n",
    "\n",
    "totalC = sum(pcaC.explained_variance_)\n",
    "print(\"---------------PCA - DATA C : EXPLAIN VARIANCE--------------\")\n",
    "print(pcaC.explained_variance_)\n",
    "print(\"---------------PCA - DATA C : SUM VARIANCE--------------\")\n",
    "print(totalC)\n",
    "var_exp_c = [(i / totalC) for i in sorted(pcaC.explained_variance_, reverse=True)]\n",
    "cum_var_exp_c = np.cumsum(var_exp_c)\n",
    "print(\"---------------PCA - DATA A : SIZE OF VARIANCE --------------\")\n",
    "print(len(var_exp_c))\n",
    "# plot explained variances\n",
    "plt.bar(range(1,8), var_exp_c, alpha=0.5,\n",
    "        align='center', label='individual explained variance')\n",
    "plt.step(range(1,8), cum_var_exp_c, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#only the first three components capture almost all the variance in the dataset. \n",
    "#So, we decide to select only the first three components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17549d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.jcchouinard.com/pca-with-python/\n",
    "sns.set()\n",
    " \n",
    "# Reduce from 5 to 3 features with PCA\n",
    "pcaC = PCA(n_components=7)\n",
    " \n",
    "# Fit and transform data\n",
    "reduced_features = pcaC.fit_transform(scaled_data_C)#featuresVector_B)\n",
    " \n",
    "# Bar plot of explained_variance\n",
    "plt.bar(\n",
    "    range(1,len(pcaC.explained_variance_)+1),\n",
    "    pcaC.explained_variance_\n",
    "    )\n",
    " \n",
    "plt.plot(\n",
    "    range(1,len(pcaC.explained_variance_ )+1),\n",
    "    np.cumsum(pcaC.explained_variance_),\n",
    "    c='red',\n",
    "    label='Cumulative Explained Variance')\n",
    " \n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance (eignenvalues)')\n",
    "plt.title('Scree plot for C')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "#pca = 3\n",
    "\n",
    "#only the first 3 components capture almost all the variance in the dataset. \n",
    "#So, we decide to select only the first 3 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48604a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_C = PCA(n_components=4)\n",
    "\n",
    "pca_features_C = pca_C.fit_transform(scaled_data_C)\n",
    " \n",
    "print('Shape before PCA: ', scaled_data_C.shape)\n",
    "print('Shape after PCA: ', pca_features_C.shape)\n",
    " \n",
    "pca_df_C = pd.DataFrame(\n",
    "    data=pca_features_C, \n",
    "    columns=['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "\n",
    "finalDf_C = pd.concat([pca_df_C, dataframeC['Class Attruibute']], axis = 1)\n",
    "\n",
    "print(\"------------------------------  PCA For C  -----------------------------\")\n",
    "finalDf_C.to_csv('MeterC_pca.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
